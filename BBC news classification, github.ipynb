{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "555aa5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 create a dataset, put all samples in one file called BBCdata.txt\n",
    "\n",
    "import os\n",
    "#import string\n",
    "\n",
    "BASE_DIR=r'databases\\bbc dataset'\n",
    "           \n",
    "LABELS=['business','entertainment','politics','sport','tech']\n",
    "\n",
    "\n",
    "with open('databases/BBCdata.txt', 'w', encoding='utf8') as outfile: \n",
    "        \n",
    "    for label in LABELS:\n",
    "            #dir='%s/%s' %(BASE_DIR, label)\n",
    "        dir=os.path.join(BASE_DIR,label)\n",
    "        for filename in os.listdir(dir):\n",
    "            fullfilename='%s/%s' %(dir, filename)\n",
    "            with open (fullfilename, 'rb') as file:\n",
    "                text=file.read().decode(errors='replace').replace('\\n',' ') \n",
    "                outfile.write('%s\\t%s\\t%s\\n' %(label,filename,text)) \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "332fa7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2  docs =[(class,txt),(class,txt),....] : convert data file to a list of tupels\n",
    "\n",
    "docs=[]\n",
    "with open ('databases/BBCdata.txt', 'r', encoding='utf8') as datafile:\n",
    "    for row in datafile:\n",
    "        parts=row.split('\\t')\n",
    "        doc=(parts[0], parts[2].strip())\n",
    "        docs.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a14c6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 text preprocessing: text cleaning\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import porter\n",
    "\n",
    "\n",
    "stemmer = porter.PorterStemmer() # stemming each word to the root\n",
    "\n",
    "newtuple=[]\n",
    "\n",
    "for doc in docs:   \n",
    "        \n",
    "    doc_label=doc[0]\n",
    "    matn=doc[1]\n",
    "    matn=matn.translate(str.maketrans('','',string.punctuation)) \n",
    "    matn=matn.lower()\n",
    "    \n",
    "    word_tokens=matn.split() \n",
    "    review = [stemmer.stem(word) for word in word_tokens if not word in set(stopwords.words('english'))]  \n",
    "    \n",
    "    review = \" \".join(review) \n",
    "    \n",
    "    newlist=(doc_label, review) \n",
    "    newtuple.append(newlist)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06935ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 classifier coniguration, and training\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split([i[1] for i in newtuple], [i[0] for i in newtuple], \n",
    "                                                    test_size=0.30, random_state=42, shuffle=True)\n",
    "    \n",
    "vectorizer=CountVectorizer(stop_words='english', ngram_range=[1,3],\n",
    "                              min_df=3, analyzer='word')\n",
    "    \n",
    "dtm=vectorizer.fit_transform(xtrain)\n",
    "NB=MultinomialNB().fit(dtm,ytrain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57eeeb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nave Bayes  Train:\t accuracy: 0.997431\t percision: 0.997431\t recall: 0.997431\t f1: 0.997431\n",
      "\n",
      "Nave Bayes  Test:\t accuracy: 0.976048\t percision: 0.976048\t recall: 0.976048\t f1: 0.976048\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5 prediction and evaluation\n",
    "\n",
    "def evaluate_classifier (title,classifier,vectorizer,xtest,ytest):\n",
    "    xtest_tfidf=vectorizer.transform(xtest)\n",
    "    y_pred=classifier.predict(xtest_tfidf)\n",
    "    \n",
    "    accuracy  = metrics.accuracy_score(ytest, y_pred)\n",
    "    precision = metrics.precision_score(ytest,y_pred, average='micro') # default average='binary'\n",
    "    recall    = metrics.recall_score(ytest,y_pred, average='micro')\n",
    "    f1        = metrics.f1_score(ytest,y_pred, average='micro')\n",
    "    \n",
    "    \n",
    "    print('%s accuracy: %f\\t percision: %f\\t recall: %f\\t f1: %f\\n' %(title,accuracy,precision,recall,f1))\n",
    "    \n",
    "evaluate_classifier('Nave Bayes  Train:\\t', NB, vectorizer, xtrain, ytrain )\n",
    "evaluate_classifier('Nave Bayes  Test:\\t',  NB, vectorizer, xtest, ytest )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed29f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the classifier and vectorizer:\n",
    "import pickle\n",
    "\n",
    "clf_filename='NB.pkl'\n",
    "pickle.dump(NB, open(clf_filename,'wb')) #  (esme model train shode, open(esme file save shavanve)), shabakeye NB_model ro be esme clf_filename='NB.pkl' zakhire mikone\n",
    "\n",
    "vec_filename='count_vectorizer.pkl'\n",
    "pickle.dump(vectorizer, open(vec_filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6b05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
